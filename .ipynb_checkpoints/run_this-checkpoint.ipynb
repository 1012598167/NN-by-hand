{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#因为之前那份一堆调试什么的 代码太乱了，所以助教你跑这个吧\n",
    "#我在这里没跑过 从头到尾跑时间太长了 所以如果有问题麻烦回去看原来那份跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('mushrooms.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map={'p':0,'e':1}\n",
    "data['class']=data['class'].map(class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(data.shape[0]):\n",
    "    for j in range(1,data.shape[1]):\n",
    "        data.iloc[i,j]=(ord(data.iloc[i,j])-ord('a'))/26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop('stalk-root',axis=1)#缺省太多的特征不去考虑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datanp=np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = datanp[:, 1:], datanp[:, 0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,y_train.shape,X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "# def load_data():\n",
    "#     f = gzip.open('../data/mnist.pkl.gz', 'rb')\n",
    "#     training_data, validation_data, testing_data = pickle.load(f,encoding='bytes')\n",
    "\n",
    "def load_data_wrapper(train_data,test_data):\n",
    "    \"\"\"Return a tuple containing ``(training_data, validation_data,\n",
    "    test_data)``. Based on ``load_data``, but the format is more\n",
    "    convenient for use in our implementation of neural networks.\n",
    "\n",
    "    In particular, ``training_data`` is a list containing 50,000\n",
    "    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n",
    "    containing the input image.  ``y`` is a 10-dimensional\n",
    "    numpy.ndarray representing the unit vector corresponding to the\n",
    "    correct digit for ``x``.\n",
    "\n",
    "    ``validation_data`` and ``test_data`` are lists containing 10,000\n",
    "    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n",
    "    numpy.ndarry containing the input image, and ``y`` is the\n",
    "    corresponding classification, i.e., the digit values (integers)\n",
    "    corresponding to ``x``.\n",
    "\n",
    "    Obviously, this means we're using slightly different formats for\n",
    "    the training data and the validation / test data.  These formats\n",
    "    turn out to be the most convenient for use in our neural network\n",
    "    code.\"\"\"\n",
    "    tr_d, te_d = train_data,test_data\n",
    "    training_inputs = [np.reshape(x, (21, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    test_inputs = [np.reshape(x, (21, 1)) for x in te_d[0]]\n",
    "    test_results = [vectorized_result(y) for y in te_d[1]]\n",
    "    test_data = zip(test_inputs, test_results)\n",
    "    training_data=list(training_data)\n",
    "    test_data=list(test_data)\n",
    "    return (training_data,test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0,1) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((2, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data,test_data=load_data_wrapper(training_data,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "class Network(object):\n",
    "    #sizes 是一个数组，其中每一个数值表示每一层的神经元的数量\n",
    "    def __init__(self, sizes):#[21,15,2]\n",
    "        self.num_layers = len(sizes)#3层\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y,1) for y in sizes[1:]]#初始化偏置\n",
    "        #weights 的shape 是 （n+1）* n的形式\n",
    "        self.weights = [np.random.randn(y,x) for x, y in zip(sizes[:-1],sizes[1:])]#21*15 15*2\n",
    "\n",
    "    def sigmoid(self,z):\n",
    "        #randn(x,y) 会生成一个(x*y)的随机数组\n",
    "        z=z.astype(float)\n",
    "        return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "    def sigmoid_prime(self,z):#sigmoid的导数\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "\n",
    "\n",
    "    def feedforward(self,a):#前馈神经网络 a input 把output输出\n",
    "        \"\"\" return the output of the network if \"a\" is input \"\"\"\n",
    "        for b, w in zip(self.biases,self.weights):\n",
    "            a = self.sigmoid(np.dot(w,a)+b)#按元素点乘\n",
    "        return a\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data = None):\n",
    "        #training_data,30,10,3.0,test_data=test_data\n",
    "        '''\n",
    "        training_data 是训练集，epochs是训练周期  \n",
    "        mini_batch_size 是mini_batch_SGD 算法中 一个组的size\n",
    "        eta 是学习速率\n",
    "        '''\n",
    "        '''\n",
    "        用小批量随机梯度下降训练神经网络。' training_data ' '是一个列表' ' (x, y) ' '表示训练输入和所需输出。其他非可选参数是不言自明的。\n",
    "        如果提供了' ' test_data ' '，则在每个历元之后根据测试数据对网络进行评估，并打印出部分进度。这是有用的跟踪进展，但大大放慢了速度。\n",
    "        '''\n",
    "        evaluation_accuracy = []\n",
    "        training_accuracy = []\n",
    "        if test_data:\n",
    "            n_test = len(test_data)#2031\n",
    "        n = len(training_data)#6093\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)#随机排\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]#0-10的对 11-20的对\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch {0}: {1} / {2}\".format(\n",
    "                    j, self.evaluate(test_data), n_test))\n",
    "            else:\n",
    "                print(\"Epoch {0} complete\".format(j))\n",
    "            accuracy = self.evaluate(training_data)\n",
    "            training_accuracy.append(accuracy)\n",
    "            accuracy = self.evaluate(test_data)\n",
    "            evaluation_accuracy.append(accuracy)\n",
    "        return  evaluation_accuracy, training_accuracy\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        '''\n",
    "        更新网络的权重和偏差应用梯度下降使用反向传播到一个单一的小批。' ' mini_batch ' '是一个元组列表' ' (x, y) ' '， ' ' eta ' '是学习率。\n",
    "        '''\n",
    "        nabla_bias = [np.zeros(b.shape) for b in self.biases]#每层大小*1 的多个零向量\n",
    "        nabla_weight = [np.zeros(w.shape) for w in self.weights]#xxx 的多个零矩阵\n",
    "        for x, y in mini_batch:\n",
    "#             print('x.shape',x.shape,'y.shape',y.shape)\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x,y) # Cx对权重和偏置的全部导数\n",
    "#             print('delta_nabla_b[0].shape',delta_nabla_b[0].shape,'delta_nabla_w[0].shape',delta_nabla_w[0].shape)\n",
    "            nabla_bias = [nb+dnb for nb, dnb in zip(nabla_bias,delta_nabla_b)]\n",
    "#             print('nabla_weight[0]:',nabla_weight[0].shape)\n",
    "#             print('delta_nabla_w[0]:',delta_nabla_w[0].shape)\n",
    "            nabla_weight = [nw + dnw for nw, dnw in zip(nabla_weight,delta_nabla_w)]\n",
    "        #更新参数\n",
    "        self.weights = [w - (eta/len(mini_batch))* nw for w, nw in zip(self.weights, nabla_weight)]\n",
    "        self.biases = [b - (eta/len(mini_batch))* nb for b, nb in zip(self.biases, nabla_bias)]#除以len(mini_batch)求所有导数的平均值\n",
    "        \n",
    "    def backprop(self,x ,y):\n",
    "        '''\n",
    "        \"返回tuple ' (nabla_b, nabla_w) ' '表示损失函数C_x的梯度。“nabla_b”和“nabla_w”是逐层的numpy array列表，类似于“self.biases and self.weights’\n",
    "        '''\n",
    "        nabla_bias = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_weight = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        activation = x\n",
    "        activations = [x] #list to store all activations, layer by layer\n",
    "        zs = [] #list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases,self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            #print('z.shape',z.shape)\n",
    "            activation = self.sigmoid(z)\n",
    "            #print('activation.shape',activation.shape)\n",
    "            activations.append(activation)\n",
    "            #print('activations',activations)\n",
    "            # 向后传播轨迹\n",
    "            #print('activations[-1]',activations)\n",
    "        delta = self.cost_derivative(activations[-1],y) * self.sigmoid_prime(zs[-1])\n",
    "#         print('delta:',delta.shape)\n",
    "        nabla_bias[-1] = delta\n",
    "        nabla_weight[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = self.sigmoid_prime(z)\n",
    "#             print(self.weights[-l+1].transpose().shape)\n",
    "#             print(delta.shape)\n",
    "#             print(sp.shape)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(),delta) * sp\n",
    "            nabla_bias[-l] = delta\n",
    "            nabla_weight[-l] = np.dot(delta, activations[-l-1].transpose())#+1改成-1\n",
    "#             print(self.weights[-l+1].transpose().shape)\n",
    "#         print(nabla_bias[0].shape)\n",
    "#         print(nabla_weight[0].shape)\n",
    "        return (nabla_bias, nabla_weight)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        for (x,y) in test_data:\n",
    "#             print('x.shape',x.shape)\n",
    "#             print('y.shape',y.shape)\n",
    "            break\n",
    "        test_results = [(np.argmax(self.feedforward(x)),np.argmax(y)) for (x,y) in test_data]#y改成np.argmax(y)\n",
    "        for (x,y) in test_results:\n",
    "#             print('x.shape',x)\n",
    "#             print('y.shape',y)\n",
    "            break\n",
    "        return sum(int(x == y) for (x,y) in test_results) \n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "\n",
    "        return(output_activations - y)\n",
    "    def save(self, filename):\n",
    "        data = {\"sizes\": self.sizes,\n",
    "                \"weights\": [w.tolist() for w in self.weights],\n",
    "                \"biases\": [b.tolist() for b in self.biases]}\n",
    "        f = open(filename, \"w\")\n",
    "        json.dump(data, f) #写入json文件\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network([21,30,30,2])\n",
    "# print(net.weights)\n",
    "net.SGD(training_data,30,10,3.0,test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save(\"model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对比 隐藏层的对比\n",
    "epochs=30\n",
    "mini_batch=10\n",
    "eta=3.0\n",
    "layers1 = [21,30,2]\n",
    "layers2 = [21,50,2]\n",
    "layers3 = [21,30,100,2]\n",
    "layers4 = [21,50,100,2]\n",
    "net1 = Network(layers1)\n",
    "net2 = Network(layers2)\n",
    "net3 = Network(layers3)\n",
    "net4 = Network(layers4)\n",
    "\n",
    "\n",
    "evaluation_accuracy1 = net1.SGD(training_data,epochs, mini_batch, eta, test_data=test_data)\n",
    "evaluation_accuracy2 = net2.SGD(training_data,epochs, mini_batch, eta, test_data=test_data)\n",
    "evaluation_accuracy3 = net3.SGD(training_data,epochs, mini_batch, eta, test_data=test_data)\n",
    "evaluation_accuracy4 = net4.SGD(training_data,epochs, mini_batch, eta, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(1,epochs+1,1), evaluation_accuracy1[0], 'x-', color='red', label = '[21,30,2]')\n",
    "ax.plot(np.arange(1,epochs+1,1), evaluation_accuracy2[0], 'o-', color='blue', label = '[21,50,2]')\n",
    "ax.plot(np.arange(1,epochs+1,1), evaluation_accuracy3[0], 'x-', color='green', label = '[21,30,100,2]')\n",
    "ax.plot(np.arange(1,epochs+1,1), evaluation_accuracy4[0], 'o-', color='yellow', label = '[21,50,100,2]')\n",
    "ax.set_xlim([1,31])\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('CompareofLayers')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('CompareofLayers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对比2 隐藏层的对比---局部图\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(18,epochs+1,1), evaluation_accuracy1[0][-13:], 'x-', color='red', label = '[21,30,2]')\n",
    "ax.plot(np.arange(18,epochs+1,1), evaluation_accuracy2[0][-13:], 'o-', color='blue', label = '[21,50,2]')\n",
    "ax.plot(np.arange(18,epochs+1,1), evaluation_accuracy3[0][-13:], 'x-', color='green', label = '[21,30,100,2]')\n",
    "ax.plot(np.arange(18,epochs+1,1), evaluation_accuracy4[0][-13:], 'o-', color='yellow', label = '[21,50,100,2]')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('CompareofLayers')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('CompareofLayers2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对比3 小批次大小\n",
    "epochs=30\n",
    "batch1 = 10\n",
    "batch2 = 20\n",
    "batch3 = 100\n",
    "batch4 = 5\n",
    "eta=3.0\n",
    "\n",
    "layers4 = [21,50,100,2]\n",
    "net1 = Network(layers4)\n",
    "net2 = Network(layers4)\n",
    "net3 = Network(layers4)\n",
    "net4 = Network(layers4)\n",
    "\n",
    "\n",
    "evaluation_accuracy1 = net1.SGD(training_data,epochs, batch1, eta, test_data=test_data)\n",
    "evaluation_accuracy2 = net2.SGD(training_data,epochs, batch2, eta, test_data=test_data)\n",
    "evaluation_accuracy3 = net3.SGD(training_data,epochs, batch3, eta, test_data=test_data)\n",
    "evaluation_accuracy4 = net4.SGD(training_data,epochs, batch4, eta, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(1,epochs+1,1), evaluation_accuracy1[0], 'x-', color='red', label = 'mini_batch=10')\n",
    "ax.plot(np.arange(1,epochs+1,1), evaluation_accuracy2[0], 'o-', color='blue', label = 'mini_batch=20')\n",
    "ax.plot(np.arange(1,epochs+1,1), evaluation_accuracy3[0], 'x-', color='green', label = 'mini_batch=100')\n",
    "ax.plot(np.arange(1,epochs+1,1), evaluation_accuracy4[0], 'o-', color='black', label = 'mini_batch=5')\n",
    "ax.set_xlim([1,31])\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('CompareofMinibatch')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('CompareofMinibatch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对比四 学习率\n",
    "\n",
    "batch=20\n",
    "epochs=30\n",
    "eta1 = 0.05\n",
    "eta2 = 0.5\n",
    "eta3 = 10\n",
    "eta4 = 50\n",
    "\n",
    "layers4 = [21,50,100,2]\n",
    "net1 = Network(layers4)\n",
    "net2 = Network(layers4)\n",
    "net3 = Network(layers4)\n",
    "net4 = Network(layers4)\n",
    "\n",
    "\n",
    "evaluation_accuracy1 = net1.SGD(training_data,epochs, batch, eta1, test_data=test_data)\n",
    "evaluation_accuracy2 = net2.SGD(training_data,epochs, batch, eta2, test_data=test_data)\n",
    "evaluation_accuracy3 = net3.SGD(training_data,epochs, batch, eta3, test_data=test_data)\n",
    "evaluation_accuracy4 = net4.SGD(training_data,epochs, batch, eta4, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(1,epochs+1,1), evaluation_accuracy1[0], 'x-', color='red', label = 'eta=0.05')\n",
    "ax.plot(np.arange(1,epochs+1,1), evaluation_accuracy2[0], 'o-', color='blue', label = 'eta=0.5')\n",
    "ax.plot(np.arange(1,epochs+1,1), evaluation_accuracy3[0], 'x-', color='green', label = 'eta=10')\n",
    "ax.plot(np.arange(1,epochs+1,1), evaluation_accuracy4[0], 'o-', color='yellow', label = 'eta=50')\n",
    "\n",
    "ax.set_xlim([1,31])\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('CompareofEta')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('CompareofEta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
